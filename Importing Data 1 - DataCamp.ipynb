{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Atom.lnk',\n",
       " 'brazilian-ecommerce',\n",
       " 'Clustering Practice .ipynb',\n",
       " 'desktop.ini',\n",
       " 'Dropbox.lnk',\n",
       " 'Importing Data 1 - DataCamp.ipynb',\n",
       " 'iv_mk_8.6.ipynb',\n",
       " 'Mall_Customers.csv',\n",
       " 'Marketing Analysis - Brazilian eCommerce.ipynb',\n",
       " 'mk_8.1.ipynb',\n",
       " 'olist_public_dataset_v2.csv',\n",
       " 'Tor Browser']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os library to explore current directory\n",
    "import os \n",
    "wd = os.getcwd() # store names of current directory in a sting wd \n",
    "os.listdir(wd) # output contentents of directory as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'moby_dick.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fdecac1ec039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Open a file: file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"moby_dick.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Print it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Check whether file is closed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'moby_dick.txt'"
     ]
    }
   ],
   "source": [
    "# Open a file: file\n",
    "file = open(\"moby_dick.txt\", mode=\"r\")\n",
    "# Print it\n",
    "print(file.read())\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "# Close file\n",
    "file.close()\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from numpy using loadtxt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read one line at a time usingthe \"with\" context manager \n",
    "\n",
    "with open(\"name.txt\") as file:\n",
    "    print (file.readline()) # 1st line\n",
    "    print (file.readline()) # 2nd line\n",
    "    print (file.readline()) # 3rd line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import this #The Zen of Python, by Tim Peters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "digits.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-024ce773c471>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"digits.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdigits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdigits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    622\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    623\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: digits.csv not found."
     ]
    }
   ],
   "source": [
    "# Loading MNIST dataset with numpy \n",
    "import numpy as np \n",
    "\n",
    "file = \"digits.csv\"\n",
    "digits = np.loadtxt(file, delimiter=\",\")\n",
    "print (digits)\n",
    "# select and reshape a row (number)\n",
    "im = digits [21,1:]\n",
    "im_sq = np.reshape(im,(28,28))\n",
    "# plot \n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im_sq,cmap=\"Greys\",interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing numpy import \n",
    "data = np.loadtxt(file, delimiter=\"\\t\",skiprows=1,usecols=[0,3]) # usecols takes a list of columns to keep, 1 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing different datatypes \n",
    "import numpy as np\n",
    "file = \"seaslug.txt\"\n",
    "\n",
    "# 1. loading file as strings, inclusing the numbers - to print them\n",
    "data = np.loadtxt(file, delimiter=\"\\t\",dtype=str)\n",
    "print (data[0])\n",
    "# 2. loading file as numbers and skip strings - to visualise them\n",
    "data_f = no.loadtxt(file, delimiter=\"\\t\",dtype=float,skiprows=1)\n",
    "print (data_f[9]) #  to print 10th element: python indexes start at 0 \n",
    "\n",
    "# scatterplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data_f[:,0]data_f[:,1])\n",
    "plt.xlabel(\"time (min.)\")\n",
    "plt.ylabel(\"% larvae\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with mixed data types \n",
    "\n",
    "file = \"titanic.csv\"\n",
    "# create a structured array (for dataset with different types)\n",
    "data = np.genfromtxt(file,delimiter=\",\",names=True,dtype=None) # when mixed data type in dataset use None to auto-assign type\n",
    "# names = headers\n",
    "print(data.shape)\n",
    "data[i] # i th row \n",
    "data[\"columnName\"] # elements in a specific column\n",
    "\n",
    "data[\"column1\"][-4:] # last 4 rows values for a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same can be done with a simpler function that has default dtype=None, delimiter=\",\",names=True\n",
    "data_easy = np.recfromcsv(\"csv_name\")\n",
    "data_easy[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = \"my.csv\"\n",
    "df = pd.read_csv(file, nrows=5, header=None) # just 5 first rows and no headers\n",
    "\n",
    "# CREATE A NUMPY ARRAY\n",
    "numpy_array = df.values\n",
    "\n",
    "print (type(numpy_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Customizing Pandas import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing other data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickled files => python objects turned in to bytes \n",
    "    # need to serialise them \n",
    "    # .pkl data type\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"data.pkl\",\"rb\") as file: # \"rb\" =>  r stands for read and b for binary \n",
    "    d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3fbcf4c5dcb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"excel_file.xlsx\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msheet_names\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# .sheet_names shows all sheets in a Excel file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()\n",
    "\n",
    "file = \"excel_file.xlsx\"\n",
    "data = pd.ExcelFile(file)\n",
    "print (data.sheet_names)  # .sheet_names shows all sheets in a Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sheets from Excel files\n",
    " # importing specific sheets as a DataFrame \n",
    "    # either use Name or Index \n",
    "\n",
    "df1 = data.parse(\"2004\") # select a sheet by name, from a loaded xls file \n",
    "df2 = data.parse(0) # select a sheet by index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customizing the spreadsheet import\n",
    "\n",
    "# 1st sheet and rename columns, skip 1st\n",
    "df3 = data.parse(0, skiprows=[1], names=[\"a\",\"b\"])\n",
    "\n",
    "# 2nd sheet and rename columns, use only 1st column, skip 1st\n",
    "df4 = data.parse(1, skiprows=[1], usecols=[0],names=[\"Country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing SAS and Stata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas7bdat import SAS7BDAT as sas\n",
    "\n",
    "with sas(\"pop.sas7bdat\") as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "pd.DataFrame.hist(df_sas[[\"P\"]])\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stata = pd.read_stata(\"stata_file.sta\")\n",
    "pd.DataFrame.hist(df_stata[[\"xxx\"]])\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imporing HDF5 \n",
    "- storing large numeric datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py # library \n",
    "file = \"zzz.hdf5\"\n",
    "data = h5py.File(file,\"r\")\n",
    "\n",
    "# print keys in data -> hierarchical  structure\n",
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = data[\"group1\"] #get hdf5 group \n",
    "\n",
    "# see keys in group\n",
    "for key in group.keys():\n",
    "    print (key)\n",
    "    \n",
    "# assign a time-series to a variable \n",
    "strain = data[\"strain\"][\"Strain\"].values  # to get values from group and key \n",
    "\n",
    "# state the number of elements in a sample \n",
    "num_sample = 10000 \n",
    "\n",
    "# set time vector \n",
    "time = np.arange (0,1, 1/num_samples)\n",
    "\n",
    "# plot \n",
    "plt.plot(time,strain[:num_samples])\n",
    "plt.xlabel(\"a\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matlab files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io \n",
    "\n",
    "mat = scipy.io.loadmat(\"xxx.mat\")\n",
    "#check what type of object \n",
    "print (type(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keys of mat dict \n",
    "mat.keys()\n",
    "# print types for key A\n",
    "type(mat[\"A\"])\n",
    "# print shape of A values \n",
    "np.shape(mat[\"A\"])\n",
    "\n",
    "# plot time-series\n",
    "data = mat[\"A\"][25,5:] # 25 rows for every column after 5\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel(\"d\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a database engine \n",
    "\n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "engine = create_engine(\"sqlite:///name_aak.sqlite\") # connect to database\n",
    "table_names = engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queriying a database\n",
    "# import packages \n",
    "from sqlalchemy import create_engine\n",
    "# create engine \n",
    "engine= create_engine(\"sqlite:/// NNAMME .sqlite\")\n",
    "#open connecting \n",
    "con = engine.connect()\n",
    "# query \n",
    "rs = con.execute(\"SELECT * FROM Album\")   # to get everything\n",
    "# save results in df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "# close conenction\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a context manager to automaticall close connection\n",
    "with engine.connect() as con:\n",
    "    rs = con.connect(\"SELECT Var1, Var2 FROM Table1\") # get specific columns \n",
    "    df = pd.DataFrame(rs.fetchmany(3)) # import 3 records only \n",
    "    df.columns = rs.keys() # name df columns from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering your database records using SQL's WHERE\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine(\"sqlite:///Chinook.sqlite\")\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee WHERE EmployeeId >= 6\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER BY\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine(\"sqlite:///Chinook.sqlite\")\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Executing SQL queries with 1 line in pandas.\n",
    "\n",
    "read_sql_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine(\"sqlite:///Chinook.sqlite\")\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas for more complex querying\n",
    "engine = create_engine(\"sqlite:///Chinook.sqlite\")\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate\",engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relationships between tables: INNER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "\n",
    "# OR \n",
    "\n",
    "df = pd.read_sql_query(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID=Artist.ArtistID\",engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query INNER JOIN and WHERE, store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000\",engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- END ---------------------- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
